{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN83wIwRQMyD1l/OBgaoV3a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edukj-wpAJ60","outputId":"602c98cd-c603-49fd-f0b6-cae0a7f2d456"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'batch': 16, 'iter': 18000, 'n_flow': 32, 'n_block': 4, 'no_lu': True, 'affine': True, 'n_bits': 5, 'lr': 0.0001, 'img_size': 64, 'temp': 0.7, 'n_sample': 20, 'path': './data'}\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-1-09322f4ae3f8>:79: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n","The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n","Q, R = torch.qr(A, some)\n","should be replaced with\n","Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2416.)\n","  q, _ = torch.qr(weight)\n","<ipython-input-1-09322f4ae3f8>:113: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n","  w_s = torch.from_numpy(w_s)\n","  0%|          | 0/18000 [00:00<?, ?it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/drive/MyDrive/Colab Notebooks/Aprendizado-Profundo/trab2-normalizing_flows/data/cifar-10-python.tar.gz\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/170498071 [00:00<?, ?it/s]\u001b[A\n","  0%|          | 98304/170498071 [00:00<04:24, 645083.70it/s]\u001b[A\n","  0%|          | 458752/170498071 [00:00<01:25, 1988315.04it/s]\u001b[A\n","  1%|          | 1835008/170498071 [00:00<00:26, 6446796.03it/s]\u001b[A\n","  4%|▎         | 6127616/170498071 [00:00<00:08, 19375188.60it/s]\u001b[A\n","  6%|▌         | 10584064/170498071 [00:00<00:05, 27804446.15it/s]\u001b[A\n","  9%|▉         | 15761408/170498071 [00:00<00:04, 35553986.15it/s]\u001b[A\n"," 12%|█▏        | 20217856/170498071 [00:00<00:03, 38185018.80it/s]\u001b[A\n"," 15%|█▍        | 24870912/170498071 [00:00<00:03, 40748581.67it/s]\u001b[A\n"," 17%|█▋        | 29261824/170498071 [00:00<00:03, 41687074.64it/s]\u001b[A\n"," 20%|██        | 34701312/170498071 [00:01<00:02, 45404588.96it/s]\u001b[A\n"," 23%|██▎       | 39288832/170498071 [00:01<00:02, 45061855.24it/s]\u001b[A\n"," 26%|██▋       | 44826624/170498071 [00:01<00:02, 46533921.33it/s]\u001b[A\n"," 30%|██▉       | 50429952/170498071 [00:01<00:02, 49148910.69it/s]\u001b[A\n"," 32%|███▏      | 55377920/170498071 [00:01<00:02, 48914284.70it/s]\u001b[A\n"," 35%|███▌      | 60293120/170498071 [00:01<00:02, 48597372.73it/s]\u001b[A\n"," 39%|███▊      | 65732608/170498071 [00:01<00:02, 50303197.56it/s]\u001b[A\n"," 42%|████▏     | 70877184/170498071 [00:01<00:01, 50551270.44it/s]\u001b[A\n"," 45%|████▍     | 76087296/170498071 [00:01<00:01, 50411351.05it/s]\u001b[A\n"," 48%|████▊     | 81952768/170498071 [00:02<00:01, 52770476.58it/s]\u001b[A\n"," 51%|█████     | 87261184/170498071 [00:02<00:01, 51777187.96it/s]\u001b[A\n"," 54%|█████▍    | 92471296/170498071 [00:02<00:01, 51222152.50it/s]\u001b[A\n"," 57%|█████▋    | 97681408/170498071 [00:02<00:01, 51460008.25it/s]\u001b[A\n"," 61%|██████    | 103186432/170498071 [00:02<00:01, 52485936.06it/s]\u001b[A\n"," 64%|██████▎   | 108462080/170498071 [00:02<00:01, 51523689.10it/s]\u001b[A\n"," 67%|██████▋   | 113639424/170498071 [00:02<00:01, 50264322.73it/s]\u001b[A\n"," 70%|███████   | 119472128/170498071 [00:02<00:00, 52521729.69it/s]\u001b[A\n"," 73%|███████▎  | 124747776/170498071 [00:02<00:00, 50748858.22it/s]\u001b[A\n"," 76%|███████▌  | 129859584/170498071 [00:02<00:00, 50303837.28it/s]\u001b[A\n"," 79%|███████▉  | 134905856/170498071 [00:03<00:00, 48460089.17it/s]\u001b[A\n"," 82%|████████▏ | 140148736/170498071 [00:03<00:00, 49568508.36it/s]\u001b[A\n"," 85%|████████▌ | 145588224/170498071 [00:03<00:00, 49507041.88it/s]\u001b[A\n"," 88%|████████▊ | 150568960/170498071 [00:03<00:00, 48625240.26it/s]\u001b[A\n"," 91%|█████████ | 155451392/170498071 [00:03<00:00, 46287842.74it/s]\u001b[A\n"," 94%|█████████▍| 160530432/170498071 [00:03<00:00, 47423251.71it/s]\u001b[A\n","100%|██████████| 170498071/170498071 [00:03<00:00, 45059038.87it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Extracting /content/drive/MyDrive/Colab Notebooks/Aprendizado-Profundo/trab2-normalizing_flows/data/cifar-10-python.tar.gz to /content/drive/MyDrive/Colab Notebooks/Aprendizado-Profundo/trab2-normalizing_flows/data\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","  0%|          | 1/18000 [00:09<45:32:03,  9.11s/it]/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","Loss: 2.43389; logP: -2.10470; logdet: 4.67082; lr: 0.0001000:   0%|          | 1/18000 [00:10<45:32:03,  9.11s/it]<ipython-input-1-09322f4ae3f8>:514: RuntimeWarning: invalid value encountered in cast\n","  img = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)  # Normalize to [0, 255]\n","Loss: 1.04534; logP: -2.63334; logdet: 6.58801; lr: 0.0001000:  34%|███▍      | 6147/18000 [2:15:14<4:24:43,  1.34s/it]"]}],"source":["from tqdm import tqdm\n","import numpy as np\n","from PIL import Image\n","from math import log, sqrt, pi\n","\n","import torch\n","from torch import nn, optim\n","from torch.autograd import Variable, grad\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from math import log, pi, exp\n","import numpy as np\n","from scipy import linalg as la\n","\n","\n","logabs = lambda x: torch.log(torch.abs(x))\n","\n","\n","class ActNorm(nn.Module):\n","    def __init__(self, in_channel, logdet=True):\n","        super().__init__()\n","\n","        self.loc = nn.Parameter(torch.zeros(1, in_channel, 1, 1))\n","        self.scale = nn.Parameter(torch.ones(1, in_channel, 1, 1))\n","\n","        self.register_buffer(\"initialized\", torch.tensor(0, dtype=torch.uint8))\n","        self.logdet = logdet\n","\n","    def initialize(self, input):\n","        with torch.no_grad():\n","            flatten = input.permute(1, 0, 2, 3).contiguous().view(input.shape[1], -1)\n","            mean = (\n","                flatten.mean(1)\n","                .unsqueeze(1)\n","                .unsqueeze(2)\n","                .unsqueeze(3)\n","                .permute(1, 0, 2, 3)\n","            )\n","            std = (\n","                flatten.std(1)\n","                .unsqueeze(1)\n","                .unsqueeze(2)\n","                .unsqueeze(3)\n","                .permute(1, 0, 2, 3)\n","            )\n","\n","            self.loc.data.copy_(-mean)\n","            self.scale.data.copy_(1 / (std + 1e-6))\n","\n","    def forward(self, input):\n","        _, _, height, width = input.shape\n","\n","        if self.initialized.item() == 0:\n","            self.initialize(input)\n","            self.initialized.fill_(1)\n","\n","        log_abs = logabs(self.scale)\n","\n","        logdet = height * width * torch.sum(log_abs)\n","\n","        if self.logdet:\n","            return self.scale * (input + self.loc), logdet\n","\n","        else:\n","            return self.scale * (input + self.loc)\n","\n","    def reverse(self, output):\n","        return output / self.scale - self.loc\n","\n","\n","class InvConv2d(nn.Module):\n","    def __init__(self, in_channel):\n","        super().__init__()\n","\n","        weight = torch.randn(in_channel, in_channel)\n","        q, _ = torch.qr(weight)\n","        weight = q.unsqueeze(2).unsqueeze(3)\n","        self.weight = nn.Parameter(weight)\n","\n","    def forward(self, input):\n","        _, _, height, width = input.shape\n","\n","        out = F.conv2d(input, self.weight)\n","        logdet = (\n","            height * width * torch.slogdet(self.weight.squeeze().double())[1].float()\n","        )\n","\n","        return out, logdet\n","\n","    def reverse(self, output):\n","        return F.conv2d(\n","            output, self.weight.squeeze().inverse().unsqueeze(2).unsqueeze(3)\n","        )\n","\n","\n","class InvConv2dLU(nn.Module):\n","    def __init__(self, in_channel):\n","        super().__init__()\n","\n","        weight = np.random.randn(in_channel, in_channel)\n","        q, _ = la.qr(weight)\n","        w_p, w_l, w_u = la.lu(q.astype(np.float32))\n","        w_s = np.diag(w_u)\n","        w_u = np.triu(w_u, 1)\n","        u_mask = np.triu(np.ones_like(w_u), 1)\n","        l_mask = u_mask.T\n","\n","        w_p = torch.from_numpy(w_p)\n","        w_l = torch.from_numpy(w_l)\n","        w_s = torch.from_numpy(w_s)\n","        w_u = torch.from_numpy(w_u)\n","\n","        self.register_buffer(\"w_p\", w_p)\n","        self.register_buffer(\"u_mask\", torch.from_numpy(u_mask))\n","        self.register_buffer(\"l_mask\", torch.from_numpy(l_mask))\n","        self.register_buffer(\"s_sign\", torch.sign(w_s))\n","        self.register_buffer(\"l_eye\", torch.eye(l_mask.shape[0]))\n","        self.w_l = nn.Parameter(w_l)\n","        self.w_s = nn.Parameter(logabs(w_s))\n","        self.w_u = nn.Parameter(w_u)\n","\n","    def forward(self, input):\n","        _, _, height, width = input.shape\n","\n","        weight = self.calc_weight()\n","\n","        out = F.conv2d(input, weight)\n","        logdet = height * width * torch.sum(self.w_s)\n","\n","        return out, logdet\n","\n","    def calc_weight(self):\n","        weight = (\n","            self.w_p\n","            @ (self.w_l * self.l_mask + self.l_eye)\n","            @ ((self.w_u * self.u_mask) + torch.diag(self.s_sign * torch.exp(self.w_s)))\n","        )\n","\n","        return weight.unsqueeze(2).unsqueeze(3)\n","\n","    def reverse(self, output):\n","        weight = self.calc_weight()\n","\n","        return F.conv2d(output, weight.squeeze().inverse().unsqueeze(2).unsqueeze(3))\n","\n","\n","class ZeroConv2d(nn.Module):\n","    def __init__(self, in_channel, out_channel, padding=1):\n","        super().__init__()\n","\n","        self.conv = nn.Conv2d(in_channel, out_channel, 3, padding=0)\n","        self.conv.weight.data.zero_()\n","        self.conv.bias.data.zero_()\n","        self.scale = nn.Parameter(torch.zeros(1, out_channel, 1, 1))\n","\n","    def forward(self, input):\n","        out = F.pad(input, [1, 1, 1, 1], value=1)\n","        out = self.conv(out)\n","        out = out * torch.exp(self.scale * 3)\n","\n","        return out\n","\n","\n","class AffineCoupling(nn.Module):\n","    def __init__(self, in_channel, filter_size=512, affine=True):\n","        super().__init__()\n","\n","        self.affine = affine\n","\n","        self.net = nn.Sequential(\n","            nn.Conv2d(in_channel // 2, filter_size, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(filter_size, filter_size, 1),\n","            nn.ReLU(inplace=True),\n","            ZeroConv2d(filter_size, in_channel if self.affine else in_channel // 2),\n","        )\n","\n","        self.net[0].weight.data.normal_(0, 0.05)\n","        self.net[0].bias.data.zero_()\n","\n","        self.net[2].weight.data.normal_(0, 0.05)\n","        self.net[2].bias.data.zero_()\n","\n","    def forward(self, input):\n","        in_a, in_b = input.chunk(2, 1)\n","\n","        if self.affine:\n","            log_s, t = self.net(in_a).chunk(2, 1)\n","            # s = torch.exp(log_s)\n","            s = F.sigmoid(log_s + 2)\n","            # out_a = s * in_a + t\n","            out_b = (in_b + t) * s\n","\n","            logdet = torch.sum(torch.log(s).view(input.shape[0], -1), 1)\n","\n","        else:\n","            net_out = self.net(in_a)\n","            out_b = in_b + net_out\n","            logdet = None\n","\n","        return torch.cat([in_a, out_b], 1), logdet\n","\n","    def reverse(self, output):\n","        out_a, out_b = output.chunk(2, 1)\n","\n","        if self.affine:\n","            log_s, t = self.net(out_a).chunk(2, 1)\n","            # s = torch.exp(log_s)\n","            s = F.sigmoid(log_s + 2)\n","            # in_a = (out_a - t) / s\n","            in_b = out_b / s - t\n","\n","        else:\n","            net_out = self.net(out_a)\n","            in_b = out_b - net_out\n","\n","        return torch.cat([out_a, in_b], 1)\n","\n","\n","class Flow(nn.Module):\n","    def __init__(self, in_channel, affine=True, conv_lu=True):\n","        super().__init__()\n","\n","        self.actnorm = ActNorm(in_channel)\n","\n","        if conv_lu:\n","            self.invconv = InvConv2dLU(in_channel)\n","\n","        else:\n","            self.invconv = InvConv2d(in_channel)\n","\n","        self.coupling = AffineCoupling(in_channel, affine=affine)\n","\n","    def forward(self, input):\n","        out, logdet = self.actnorm(input)\n","        out, det1 = self.invconv(out)\n","        out, det2 = self.coupling(out)\n","\n","        logdet = logdet + det1\n","        if det2 is not None:\n","            logdet = logdet + det2\n","\n","        return out, logdet\n","\n","    def reverse(self, output):\n","        input = self.coupling.reverse(output)\n","        input = self.invconv.reverse(input)\n","        input = self.actnorm.reverse(input)\n","\n","        return input\n","\n","\n","def gaussian_log_p(x, mean, log_sd):\n","    return -0.5 * log(2 * pi) - log_sd - 0.5 * (x - mean) ** 2 / torch.exp(2 * log_sd)\n","\n","\n","def gaussian_sample(eps, mean, log_sd):\n","    return mean + torch.exp(log_sd) * eps\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, in_channel, n_flow, split=True, affine=True, conv_lu=True):\n","        super().__init__()\n","\n","        squeeze_dim = in_channel * 4\n","\n","        self.flows = nn.ModuleList()\n","        for i in range(n_flow):\n","            self.flows.append(Flow(squeeze_dim, affine=affine, conv_lu=conv_lu))\n","\n","        self.split = split\n","\n","        if split:\n","            self.prior = ZeroConv2d(in_channel * 2, in_channel * 4)\n","\n","        else:\n","            self.prior = ZeroConv2d(in_channel * 4, in_channel * 8)\n","\n","    def forward(self, input):\n","        b_size, n_channel, height, width = input.shape\n","        squeezed = input.view(b_size, n_channel, height // 2, 2, width // 2, 2)\n","        squeezed = squeezed.permute(0, 1, 3, 5, 2, 4)\n","        out = squeezed.contiguous().view(b_size, n_channel * 4, height // 2, width // 2)\n","\n","        logdet = 0\n","\n","        for flow in self.flows:\n","            out, det = flow(out)\n","            logdet = logdet + det\n","\n","        if self.split:\n","            out, z_new = out.chunk(2, 1)\n","            mean, log_sd = self.prior(out).chunk(2, 1)\n","            log_p = gaussian_log_p(z_new, mean, log_sd)\n","            log_p = log_p.view(b_size, -1).sum(1)\n","\n","        else:\n","            zero = torch.zeros_like(out)\n","            mean, log_sd = self.prior(zero).chunk(2, 1)\n","            log_p = gaussian_log_p(out, mean, log_sd)\n","            log_p = log_p.view(b_size, -1).sum(1)\n","            z_new = out\n","\n","        return out, logdet, log_p, z_new\n","\n","    def reverse(self, output, eps=None, reconstruct=False):\n","        input = output\n","\n","        if reconstruct:\n","            if self.split:\n","                input = torch.cat([output, eps], 1)\n","\n","            else:\n","                input = eps\n","\n","        else:\n","            if self.split:\n","                mean, log_sd = self.prior(input).chunk(2, 1)\n","                z = gaussian_sample(eps, mean, log_sd)\n","                input = torch.cat([output, z], 1)\n","\n","            else:\n","                zero = torch.zeros_like(input)\n","                # zero = F.pad(zero, [1, 1, 1, 1], value=1)\n","                mean, log_sd = self.prior(zero).chunk(2, 1)\n","                z = gaussian_sample(eps, mean, log_sd)\n","                input = z\n","\n","        for flow in self.flows[::-1]:\n","            input = flow.reverse(input)\n","\n","        b_size, n_channel, height, width = input.shape\n","\n","        unsqueezed = input.view(b_size, n_channel // 4, 2, 2, height, width)\n","        unsqueezed = unsqueezed.permute(0, 1, 4, 2, 5, 3)\n","        unsqueezed = unsqueezed.contiguous().view(\n","            b_size, n_channel // 4, height * 2, width * 2\n","        )\n","\n","        return unsqueezed\n","\n","\n","class Glow(nn.Module):\n","    def __init__(\n","        self, in_channel, n_flow, n_block, affine=True, conv_lu=True\n","    ):\n","        super().__init__()\n","\n","        self.blocks = nn.ModuleList()\n","        n_channel = in_channel\n","        for i in range(n_block - 1):\n","            self.blocks.append(Block(n_channel, n_flow, affine=affine, conv_lu=conv_lu))\n","            n_channel *= 2\n","        self.blocks.append(Block(n_channel, n_flow, split=False, affine=affine))\n","\n","    def forward(self, input):\n","        log_p_sum = 0\n","        logdet = 0\n","        out = input\n","        z_outs = []\n","\n","        for block in self.blocks:\n","            out, det, log_p, z_new = block(out)\n","            z_outs.append(z_new)\n","            logdet = logdet + det\n","\n","            if log_p is not None:\n","                log_p_sum = log_p_sum + log_p\n","\n","        return log_p_sum, logdet, z_outs\n","\n","    def reverse(self, z_list, reconstruct=False):\n","        for i, block in enumerate(self.blocks[::-1]):\n","            if i == 0:\n","                input = block.reverse(z_list[-1], z_list[-1], reconstruct=reconstruct)\n","\n","            else:\n","                input = block.reverse(input, z_list[-(i + 1)], reconstruct=reconstruct)\n","\n","        return input\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","parser = {}\n","parser[\"batch\"]=16\n","parser[\"iter\"]=default=18000\n","parser[\"n_flow\"]= 32\n","parser[\"n_block\"]= 4\n","parser[\"no_lu\"]=True\n","parser[\"affine\"]= action=True\n","parser[\"n_bits\"]= 5\n","parser[\"lr\"]= 1e-4\n","parser[\"img_size\"]= 64\n","parser[\"temp\"]= 0.7\n","parser[\"n_sample\"]= 20\n","parser[\"path\"]=(r\"./data\")\n","\n","\n","def sample_data(path, batch_size, image_size):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize(image_size),\n","            transforms.CenterCrop(image_size),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","\n","    dataset = datasets.CIFAR10((r\"/content/drive/MyDrive/Colab Notebooks/Aprendizado-Profundo/trab2-normalizing_flows/data\"), transform=transform, download=True)\n","    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=4)\n","    loader = iter(loader)\n","\n","    while True:\n","        try:\n","            yield next(loader)\n","\n","        except StopIteration:\n","            loader = DataLoader(\n","                dataset, shuffle=True, batch_size=batch_size, num_workers=4\n","            )\n","            loader = iter(loader)\n","            yield next(loader)\n","\n","\n","def calc_z_shapes(n_channel, input_size, n_flow, n_block):\n","    z_shapes = []\n","\n","    for i in range(n_block - 1):\n","        input_size //= 2\n","        n_channel *= 2\n","\n","        z_shapes.append((n_channel, input_size, input_size))\n","\n","    input_size //= 2\n","    z_shapes.append((n_channel * 4, input_size, input_size))\n","\n","    return z_shapes\n","\n","\n","def calc_loss(log_p, logdet, image_size, n_bins):\n","    # log_p = calc_log_p([z_list])\n","    n_pixel = image_size * image_size * 3\n","\n","    loss = -log(n_bins) * n_pixel\n","    loss = loss + logdet + log_p\n","\n","    return (\n","        (-loss / (log(2) * n_pixel)).mean(),\n","        (log_p / (log(2) * n_pixel)).mean(),\n","        (logdet / (log(2) * n_pixel)).mean(),\n","    )\n","\n","\n","def train(args, model, optimizer):\n","    dataset = iter(sample_data(args[\"path\"], args[\"batch\"], args[\"img_size\"]))\n","    n_bins = 2.0 ** args[\"n_bits\"]\n","\n","    z_sample = []\n","    z_shapes = calc_z_shapes(3, args[\"img_size\"], args[\"n_flow\"], args[\"n_block\"])\n","    for z in z_shapes:\n","        z_new = torch.randn(args[\"n_sample\"], *z) * args[\"temp\"]\n","        z_sample.append(z_new.to(device))\n","\n","    with tqdm(range(args[\"iter\"])) as pbar:\n","        for i in pbar:\n","            best_loss = float('inf')\n","\n","            image, _ = next(dataset)\n","            image = image.to(device)\n","\n","            image = image * 255\n","\n","            if args[\"n_bits\"] < 8:\n","                image = torch.floor(image / 2 ** (8 - args[\"n_bits\"]))\n","\n","            image = image / n_bins - 0.5\n","\n","            if i == 0:\n","                with torch.no_grad():\n","                    log_p, logdet, _ = model.module(\n","                        image + torch.rand_like(image) / n_bins\n","                    )\n","\n","                    continue\n","\n","            else:\n","                log_p, logdet, _ = model(image + torch.rand_like(image) / n_bins)\n","\n","            logdet = logdet.mean()\n","\n","            loss, log_p, log_det = calc_loss(log_p, logdet, args[\"img_size\"], n_bins)\n","            model.zero_grad()\n","            loss.backward()\n","            # warmup_lr = args[\"lr\"] * min(1, i * batch_size / (50000 * 10))\n","            warmup_lr = args[\"lr\"]\n","            optimizer.param_groups[0][\"lr\"] = warmup_lr\n","            optimizer.step()\n","\n","            pbar.set_description(\n","                f\"Loss: {loss.item():.5f}; logP: {log_p.item():.5f}; logdet: {log_det.item():.5f}; lr: {warmup_lr:.7f}\"\n","            )\n","\n","            if loss < best_loss:\n","                with torch.no_grad():\n","                    # Get the image tensor and convert it to a NumPy array\n","                    img_tensor = model_single.reverse(z_sample).cpu().data\n","                    img_numpy = img_tensor.numpy()\n","\n","                    for j in range(img_numpy.shape[0]):\n","                        img = img_numpy[j].transpose(1, 2, 0)  # Convert to (H, W, C) format\n","                        img = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)  # Normalize to [0, 255]\n","                        img_pil = Image.fromarray(img)\n","\n","                        img_pil.save(f\"/content/drive/MyDrive/Colab Notebooks/Aprendizado-Profundo/trab2-normalizing_flows/sample/{str(i + 1).zfill(6)}_{j}.png\")\n","\n","            if loss < best_loss:\n","                best_loss = loss\n","                best_model_state = model.state_dict()\n","                best_optimizer_state = optimizer.state_dict()\n","                torch.save(best_model_state, r\"/content/drive/MyDrive/Colab Notebooks/Aprendizado-Profundo/trab2-normalizing_flows/checkpoint/model.pth\")\n","                torch.save(best_optimizer_state, r\"/content/drive/MyDrive/Colab Notebooks/Aprendizado-Profundo/trab2-normalizing_flows/checkpoint/optimizer.pth\")\n","\n","\n","\n","if __name__ == \"__main__\":\n","    args = parser\n","    print(args)\n","\n","    model_single = Glow(\n","        3, args[\"n_flow\"], args[\"n_block\"], affine=args[\"affine\"], conv_lu=not args[\"no_lu\"]\n","    )\n","    model = nn.DataParallel(model_single)\n","    # model = model_single\n","    model = model.to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=args[\"lr\"])\n","\n","    train(args, model, optimizer)"]}]}